@article{guyon-elisseeff-03,
  title   = "An Introduction to Variable and Feature Selection",
  author  = "I. Guyon and A. Elisseeff",
  journal = "JMLR",
  volume  = "3",
  month   = MAR,
  pages   = "1157-1182",
  year    = 2003
}

@techreport{guyon2007causalreport,
  author      = {I. Guyon and C. Aliferis and A. Elisseeff},
  title       = {Causal Feature Selection},
  institution = {Clopinet},
  year        = 2007,
  type        = {Technical Report },
  source      = {\url{http://clopinet.com/isabelle/Papers/causalFS.pdf}}
}

%

@article{neal2011mcmc,
  title={{MCMC} using {H}amiltonian dynamics},
  author={Neal, Radford M and others},
  journal={Handbook of {M}arkov chain {M}onte {C}arlo},
  volume={2},
  number={11},
  pages={2},
  year={2011}
}

@article{carpenter2017stan,
  title={Stan: A probabilistic programming language},
  author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  journal={Journal of statistical software},
  volume={76},
  number={1},
  year={2017},
  publisher={Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA~…}
}

@article{hoffman2014no,
  title={The {No-U-Turn} sampler: adaptively setting path lengths in {H}amiltonian {M}onte {C}arlo.},
  author={Hoffman, Matthew D and Gelman, Andrew},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1593--1623},
  year={2014}
}

@article{betancourt2017conceptual,
  title={A conceptual introduction to {H}amiltonian {M}onte {C}arlo},
  author={Betancourt, Michael},
  journal={arXiv preprint arXiv:1701.02434},
  year={2017}
}

@InProceedings{pmlr-v84-ge18b,
  title = 	 {Turing: A Language for Flexible Probabilistic Inference},
  author = 	 {Hong Ge and Kai Xu and Zoubin Ghahramani},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1682--1690},
  year = 	 {2018},
  editor = 	 {Amos Storkey and Fernando Perez-Cruz},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Playa Blanca, Lanzarote, Canary Islands},
  month = 	 {09--11 Apr},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/ge18b/ge18b.pdf},
  url = 	 {http://proceedings.mlr.press/v84/ge18b.html},
  abstract = 	 {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference engines. In this work, we present a system called Turing for building MCMC algorithms for probabilistic programming inference. Turing has a very simple syntax and makes full use of the numerical capabilities in the Julia programming language, including all implemented probability distributions, and automatic differentiation. Turing supports a wide range of popular Monte Carlo algorithms, including Hamiltonian Monte Carlo (HMC), HMC with No-U-Turns (NUTS), Gibbs sampling, sequential Monte Carlo (SMC), and several particle MCMC (PMCMC) samplers. Most importantly, Turing inference is composable: it combines MCMC operations on subsets of variables, for example using a combination of an HMC engine and a particle Gibbs (PG) engine.  We explore several combinations of inference methods with the aim of finding approaches that are both efficient and universal, i.e. applicable to arbitrary probabilistic models. NUTS—a popular variant of HMC that adapts Hamiltonian simulation path length automatically, although quite powerful for exploring differentiable target distributions, is however not universal. We identify some failure modes for the NUTS engine, and demonstrate that composition of PG (for discrete variables) and NUTS (for continuous variables) can be useful when the NUTS engine is either not applicable, or simply does not work well. Our aim is to present Turing and its composable inference engines to the world and encourage other researchers to build on this system to help advance the field of probabilistic machine learning. }
}