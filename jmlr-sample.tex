 % use the "wcp" class option for workshop and conference
 % proceedings
 %\documentclass[gray]{jmlr} % test grayscale version
 %\documentclass[tablecaption=bottom]{jmlr}% journal article
 \documentclass[tablecaption=bottom,wcp]{jmlr} % W&CP article

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
 %\usepackage{longtable}% for long tables

 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
\usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}

 % Define an unnumbered theorem just for this sample document for
 % illustrative purposes:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

% Our packages
\usepackage{listings}
\lstdefinelanguage{julia}
{
  keywordsprefix=\@,
  morekeywords={
    exit,whos,edit,load,is,isa,isequal,typeof,tuple,ntuple,uid,hash,finalizer,convert,promote,
    subtype,typemin,typemax,realmin,realmax,sizeof,eps,promote_type,method_exists,applicable,
    invoke,dlopen,dlsym,system,error,throw,assert,new,Inf,Nan,pi,im,begin,while,for,in,return,
    break,continue,macro,quote,let,if,elseif,else,try,catch,end,bitstype,ccall,do,using,module,
    import,export,importall,baremodule,immutable,local,global,const,Bool,Int,Int8,Int16,Int32,
    Int64,Uint,Uint8,Uint16,Uint32,Uint64,Float32,Float64,Complex64,Complex128,Any,Nothing,None,
    function,type,typealias,abstract
  },
  sensitive=true,
  morecomment=[l]{\#},
  morestring=[b]',
  morestring=[b]" 
}

\definecolor{mygray}{RGB}{128,128,128}
\definecolor{myblue}{RGB}{0, 0, 255}
\definecolor{myolivegreen}{RGB}{186, 184, 108}
\definecolor{mymaroon}{RGB}{128, 0, 0}

\lstset{
    language=julia,
    basicstyle=\small\ttfamily, 
    columns=fullflexible, % make sure to use fixed-width font, CM typewriter is NOT fixed width
    numbers=left, 
    numberstyle=\small\ttfamily\color{mygray},
    stepnumber=1,              
    numbersep=10pt, 
    numberfirstline=true, 
    numberblanklines=true, 
    tabsize=4,
    lineskip=-1.5pt,
    extendedchars=true,
    breaklines=true,        
    keywordstyle=\color{myblue}\bfseries,
    identifierstyle=, % using emph or index keywords
    commentstyle=\sffamily\color{myolivegreen},
    stringstyle=\color{mymaroon},
    showstringspaces=false,
    showtabs=false,
    upquote=false
}

\usepackage{adjustbox}
\usepackage{multirow}

\def\ahmc{\texttt{AHMC} }
\def\ahmcs{\texttt{AHMC}'s }
\def\ahmcfull{\texttt{AdvancedHMC.jl} }
\def\ahmcfulls{\texttt{AdvancedHMC.jl}'s }
\def\stan{\texttt{Stan} }
\def\stans{\texttt{Stan}'s }
\def\turing{\texttt{Turing} }
\def\turings{\texttt{Turing}'s }
\def\turingjl{\texttt{Turing.jl} }
\def\turingjls{\texttt{Turing.jl}'s }

%%%

\jmlrproceedings{AABI 2019}{2nd Symposium on Advances in Approximate Bayesian Inference, 2019}

 % The optional argument of \title is used in the header
\title[\ahmcfull:\\\large A modular implementation of Stan’s no-U-turn sampler]{\ahmcfull:\\\large A modular implementation of Stan’s no-U-turn sampler in Julia}

 % Anything in the title that should appear in the main title but 
 % not in the article's header or the volume's table of
 % contents should be placed inside \titletag{}

 %\title{Title of the Article\titletag{\thanks{Some footnote}}}


 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % \thanks must come after \Name{...} not inside the argument for
 % example \Name{John Smith}\nametag{\thanks{A note}} NOT \Name{John
 % Smith\thanks{A note}}

 % Anything in the name that should appear in the title but not in the 
 % article's header or footer or in the volume's
 % table of contents should be placed inside \nametag{}

% Anonymous authors (leave as is; do not reveal author names for your submission)
\author{\Name{Anonymous Authors}\\
  \addr Anonymous Institution}
% THE SUBMISSION MUST REMAIN ANONYMOUS

% Two authors with the same address
% \author{\Name{Author Name1\nametag{\thanks{A note}}} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

 % Three or more authors with the same address:
 % \author{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \Name{Author Name4} \Email{an4@sample.com}\\
 %  \Name{Author Name5} \Email{an5@sample.com}\\
 %  \Name{Author Name6} \Email{an6@sample.com}\\
 %  \Name{Author Name7} \Email{an7@sample.com}\\
 %  \Name{Author Name8} \Email{an8@sample.com}\\
 %  \Name{Author Name9} \Email{an9@sample.com}\\
 %  \Name{Author Name10} \Email{an10@sample.com}\\
 %  \Name{Author Name11} \Email{an11@sample.com}\\
 %  \Name{Author Name12} \Email{an12@sample.com}\\
 %  \Name{Author Name13} \Email{an13@sample.com}\\
 %  \Name{Author Name14} \Email{an14@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:
 % \author{\Name{Author Name1} \Email{abc@sample.com}\\
 % \addr Address 1
 % \AND
 % \Name{Author Name2} \Email{xyz@sample.com}\\
 % \addr Address 2
 %}



\begin{document}

\maketitle

\begin{abstract}
The No-U-Turn Sampler (NUTS) in \stan \citep{hoffman2014no,carpenter2017stan} has demonstrated remarkable sampling robustness and efficiency in a wide range of Bayesian inference problems, due to the use of dynamic Hamiltonian trajectory and a fine-tuned joint adaptation of step-size and mass matrix. Motivated by these successes, we present \ahmcfull (\ahmc), a pure Julia implementation of \stans built-in NUTS algorithm and related adaptation methods. We hope \ahmc can help expose \stans NUTS to a wider range of users, e.g. those who want to write their models by hand or use a different probabilistic programming language, such as \texttt{Turing.jl}, \texttt{Soss.jl}. In our package, NUTS is defined as a combination of individual components with abstractions partially inspired by \citep{betancourt2017conceptual}. As being pure Julia, \ahmc also runs on GPUs by utilising \texttt{CuArrays.jl}.
\end{abstract}

% Keywords may be removed
%\begin{keywords}
%List of keywords
%\end{keywords}

\section{Introduction}
\label{sec:intro}

Hamiltonian Monte Carlo (HMC) simulates Hamiltonian dynamics to make proposals for a Markov chain \citep{neal2011mcmc}.
\texttt{AdvancedHMC.jl} supports various HMC samplers below.
$$
(\texttt{StaticHMC} \cup \texttt{DynamicHMC}) \times \texttt{Adaptor}.
$$
Here $\texttt{StaticHMC}$ are HMC with fixed-length trajectories and 
$\texttt{DynamicHMC}$ are HMC with adaptive trajectory length 
which can be created by composing NUTS components as follows
$$
\texttt{Metric} \times \texttt{Integrator} \times \texttt{TrajectorySampler} \times \texttt{TerminationCriterion},
$$
\begin{equation*}
\text{where}
    \begin{aligned} 
        \texttt{Metric} &=  \{ \texttt{UnitEuclidean}, \texttt{DiagEuclidean}, \texttt{DenseEuclidean} \} \\ 
        \texttt{Integrator} &=  \{ \texttt{Leapfrog}, \texttt{JitteredLeapfrog}, \texttt{TemperedLeapfrog} \} \\
        \texttt{TrajectorySampler} &=  \{ \texttt{Slice}, \texttt{Multinomial} \} \\
        \texttt{TerminationCriterion} &= \{ \texttt{ClassicNoUTurn}, \texttt{GeneralisedNoUTurn} \}
    \end{aligned}.
\end{equation*}
$\texttt{Adaptor}$ can be composed from base adaptors
$$
\texttt{BaseAdaptor} \in \{\texttt{Preconditioner}, \texttt{NesterovDualAveraging}\}.
$$
$\texttt{Preconditioner}$ behaves differently based on the choice of metric spaces. We also provide $\texttt{StanHMCAdaptor}$, a specific composition of base adaptors that is equivalent to \stans windowed adaptor, which has been proved to be robust in practice in \stan.

\subsection{Example code of building \stans NUTS using \ahmc}
\lstinputlisting{example.jl}

\section{Evaluations}
We consider five models from \texttt{MCMCBenchmarks.jl}, a package for benchmarking probabilistic programming languages (PPLs) in Julia, in order to compare 
NUTS between \ahmc and \texttt{Stan}.

\textbf{Gaussian Model (Gaussian)}
is a simple two parameter Gaussian distribution.
$$
\mu \sim \mathcal{N}(0, 1), \quad 
\sigma \sim \mathcal{T}runcated(\mathcal{C}auchy(0, 5), 0, \infty), \quad
y_n \sim \mathcal{N}(\mu, \sigma) \; (n = 1, \dots, N)
$$

\textbf{Signal Detection Model (SDT)}
is a model used in psychophysics and signal processing, 
which decomposes performance in terms of discriminability and bias.
$$
d \sim \mathcal{N}(0, \frac{1}{\sqrt{2}}), \quad 
c \sim \mathcal{N}(0, \frac{1}{\sqrt{2}}), \quad 
x \sim \text{SDT}(d, c).
$$

\textbf{Linear Regression Model (LR)}
is a linear regression with truncated Cauchy prior on the weights.
$$
B_d \sim \mathcal{N}(0, 10), \;
\sigma \sim \mathcal{T}runcated(\mathcal{C}auchy(0, 5), 0, \infty), \;
y_n \sim \mathcal{N}(\mu_n, \sigma),
$$
where $\mu = B_0 + B^T X, d = 1, \dots, D$ and $n = 1, \dots, N$.

\textbf{Hierarchical Poisson Regression (HPR)}
$$
a_0 \sim \mathcal{N}(0, 10), \;
a_1 \sim \mathcal{N}(0, 1), \;
b_\sigma \sim \mathcal{T}runcated(\mathcal{C}auchy(0, 1), 0, \infty), \;
b_d \sim \mathcal{N}(0, b_\sigma), \;
y_n \sim \mathcal{P}oi(\log\lambda_n),
$$
where $\log\lambda_n = a_0 + b_{z_n} + a_1 x_n, d = 1, \dots, N_b$ and $n = 1, \dots, N$.

\textbf{Linear Ballistic Accumulator (LBA)}
is a cognitive model of perception and simple decision making.
$$
\tau \sim \mathcal{T}runcated(\mathcal{N}(0.4, 0.1), 0, mn), \quad
A \sim \mathcal{T}runcated(\mathcal{N}(0.8, 0.4), 0, \infty),
$$
$$
k \sim \mathcal{T}runcated(\mathcal{N}(0.2, 0.3), 0, \infty), \quad
\nu_d \sim \mathcal{T}runcated(\mathcal{N}(0, 3), 0, \infty), \quad
x_n \sim \text{LBA}(\nu, \tau, A, k)
$$
where $mn=\min_i x_{i,2}, d = 1, \dots, N_c$ and $n = 1, \dots, N$.

\subsection{Statistical efficiency: \stans NUTS v.s. \ahmcs NUTS}
To compare the statistical efficiency between \texttt{Stan} and \texttt{AHMC},
we run multiple chains of NUTS with target acceptance rate $0.8$ 
for $2,000$ steps with $1,000$ adaptation steps,
where the warm-up samples are dropped.
We compare the simulated trajectories by the distributions of step size and tree depth and by the mean effective sample size (ESS) for all variables, shown in Figure~\ref{fig:gauss} to Figure~\ref{fig:lba}.
\begin{figure}[ht]
    \includegraphics[width=0.25\textwidth]{images/Gaussian/density_epsilon.pdf}
    \includegraphics[width=0.25\textwidth]{images/Gaussian/density_tree_depth.pdf}
    \;\hfill
    \raisebox{\height}{\scalebox{0.6}{
        \begin{tabular}{lrrr}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{$N$} & \multicolumn{2}{c}{ESS} \\
            & & \multicolumn{1}{c}{$\mu$} & \multicolumn{1}{c}{$\sigma$} \\
            \midrule
            \texttt{Stan} & 10 & 513.163 & 466.577 \\
            \texttt{AHMC} & 10 & 503.535 & 447.722 \\
            \texttt{Stan} & 100 & 786.531 & 782.231 \\
            \texttt{AHMC} & 100 & 786.531 & 796.628 \\
            \texttt{Stan} & 1000 & 864.010 & 876.660 \\
            \texttt{AHMC} & 1000 & 832.255 & 844.452 \\
            \bottomrule
        \end{tabular}
    }}\hfill\;
    \caption{Gaussian (50 runs); left to right: step size, tree depth, ESS}
    \label{fig:gauss}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.25\textwidth]{images/SDT/density_epsilon.pdf}
    \includegraphics[width=0.25\textwidth]{images/SDT/density_tree_depth.pdf}
    \;\hfill
    \raisebox{\height}{\scalebox{0.6}{
        \begin{tabular}{lrrr}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{$N$} & \multicolumn{2}{c}{ESS} \\
            & & \multicolumn{1}{c}{$d$} & \multicolumn{1}{c}{$c$} \\
            \midrule
            \texttt{Stan} & 10 & 710.762  & 703.327 \\
            \texttt{AHMC} & 10 & 802.236 & 815.929 \\
            \texttt{Stan} & 100 & 820.741 & 823.152 \\
            \texttt{AHMC} & 100 & 814.308 & 846.357 \\
            \texttt{Stan} & 1000 & 844.478 & 872.961 \\
            \texttt{AHMC} & 1000 & 829.792 & 859.018 \\
            \bottomrule
        \end{tabular}
    }}\hfill\;
    \caption{SDT (100 runs); left to right: step size, tree depth, ESS}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.25\textwidth]{images/Linear_Regression/density_epsilon.pdf}
    \includegraphics[width=0.25\textwidth]{images/Linear_Regression/density_tree_depth.pdf}
    \;\hfill
    \raisebox{\height}{\scalebox{0.6}{
        \begin{tabular}{lrrrrr}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{$N$} & \multicolumn{4}{c}{ESS} \\
            & & \multicolumn{1}{c}{$b_0$} & \multicolumn{1}{c}{$\sigma$} & \multicolumn{1}{c}{$b_1$} & \multicolumn{1}{c}{$b_2$} \\
            \midrule
            \texttt{Stan} & 10 & 413.939 & 266.476 & 381.219 & 423.441 \\
            \texttt{AHMC} & 10 & 354.946 & 263.894 & 411.769 & 399.420 \\
            \texttt{Stan} & 100 & 621.796 & 729.812 & 465.990 & 608.608 \\
            \texttt{AHMC} & 100 & 473.005 & 734.189 & 606.996 & 621.543 \\
            \texttt{Stan} & 1000 & 668.524 & 789.987 & 464.459 & 648.201 \\
            \texttt{AHMC} & 1000 & 485.988 & 786.577 & 676.097 & 689.344 \\
            \bottomrule
        \end{tabular}
    }}\hfill\;
    \caption{LR (50 runs); left to right: step size, tree depth, ESS}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.25\textwidth]{images/Hierarchical_Poisson/density_epsilon.pdf}
    \includegraphics[width=0.25\textwidth]{images/Hierarchical_Poisson/density_tree_depth.pdf}
    \;\hfill
    \raisebox{\height}{\scalebox{0.6}{
        \begin{tabular}{lrrrr}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{$N$} & \multicolumn{3}{c}{ESS} \\
            & & \multicolumn{1}{c}{$a_0$} & \multicolumn{1}{c}{$a_1$} & \multicolumn{1}{c}{$b_\sigma$} \\
            \midrule
            \texttt{Stan} & 10 & 221.485 & 215.013 & 266.900 \\
            \texttt{AHMC} & 10 & 216.491 & 214.459 & 258.638 \\
            \texttt{Stan} & 20 & 208.286 & 207.041 & 241.080 \\
            \texttt{AHMC} & 20 & 206.458 & 200.469 & 236.546 \\
            \texttt{Stan} & 50 & 172.484 & 172.982 & 216.586 \\
            \texttt{AHMC} & 50 & 200.755 & 201.548 & 247.384 \\
            \bottomrule
        \end{tabular}
    }}\hfill\;
    \caption{HPR (25 runs); left to right: step size, tree depth, ESS (of some variables)}
\end{figure}

\begin{figure}[ht]
    \includegraphics[width=0.25\textwidth]{images/LBA/density_epsilon.pdf}
    \includegraphics[width=0.25\textwidth]{images/LBA/density_tree_depth.pdf}
    \;\hfill
    \raisebox{\height}{\scalebox{0.6}{
        \begin{tabular}{lrrrrr}
            \toprule
            \multirow{2}{*}{} & \multirow{2}{*}{$N$} & \multicolumn{4}{c}{ESS} \\
            & & \multicolumn{1}{c}{$\tau$} & \multicolumn{1}{c}{$A$} & \multicolumn{1}{c}{$\nu_1$} & \multicolumn{1}{c}{$\nu_2$} \\
            \midrule
            \texttt{Stan} & 10 & 226.463 & 282.656 & 305.614 & 276.557 \\
            \texttt{AHMC} & 10 & 340.722 & 304.523 & 337.610 & 336.357 \\
            \texttt{Stan} & 50 & 212.838 & 238.003 & 235.009 & 232.667 \\
            \texttt{AHMC} & 50 & 248.249 & 238.979 & 248.331 & 255.421 \\
            \texttt{Stan} & 200 & 244.926 & 264.967 & 268.793 & 270.36 \\
            \texttt{AHMC} & 200 & 256.638 & 263.098 & 270.978 & 266.769 \\
            \bottomrule
        \end{tabular}
    }}\hfill\;
    \caption{LBA (50 runs); left to right: step size, tree depth, ESS (of some variables)}
    \label{fig:lba}
\end{figure}

\subsection{Computational efficiency: \stan v.s. \turing}
\turing \citep{pmlr-v84-ge18b} is a PPL in Julia that
uses \ahmcfull as its HMC backend.
All the benchmark models used in the paper are written in \turing,
which then calls \ahmc to run NUTS.
Below is an example of running NUTS on the LR model using \turing.
\lstinputlisting{lr.jl}    
The time to run the five benchmark models are reported in Table below.
\begin{table}
\caption{Time comparisons between \texttt{Stan} and \texttt{Turing} (\texttt{AHMC}) for five models using $^1$ 25 runs, $^2$ 50 runs or $^3$ 100 runs.} 
\label{tb:}
\centering
\scalebox{0.9}{
\begin{tabular}{lrrrrrrrrrr}
    \toprule
    %\multirow{2}{*}{} & \multicolumn{10}{c}{Data Size \& Mean Time (s)} \\
    & \multicolumn{2}{c}{Gaussian $^2$} & \multicolumn{2}{c}{SDT $^3$} & \multicolumn{2}{c}{LR $^2$} & \multicolumn{2}{c}{HPR $^1$} & \multicolumn{2}{c}{LBA $^2$} \\
    & \multicolumn{1}{c}{$N$} & \multicolumn{1}{c}{seconds} & \multicolumn{1}{c}{$N$} & \multicolumn{1}{c}{seconds} & \multicolumn{1}{c}{$N$} & \multicolumn{1}{c}{seconds}& \multicolumn{1}{c}{$N$} & \multicolumn{1}{c}{seconds}& \multicolumn{1}{c}{$N$} & \multicolumn{1}{c}{seconds} \\
    \midrule
    %\texttt{Stan} & 10 & 0.803881 & 10 & 0.775959 & 10 & 0.866924 & 10 & 2.487 & 10 & 1.91799 \\
    \texttt{Stan} & 10 & 0.8039 & 10 & 0.7759 & 10 & 0.8669 & 10 & 2.4870 & 10 & 1.9179 \\
    %Turing & 10 & 0.336073 & 10 & 0.328538 & 10 & 1.13563 & 10 & 19.4587 & 10 & 2.69062 \\
    \texttt{AHMC} & 10 & 0.3361 & 10 & 0.3285 & 10 & 1.1356 & 10 & 19.4587 & 10 & 2.6906 \\
    %\texttt{Stan} & 100 & 0.756092 & 100 & 0.726106 & 100 & 0.982419 & 20 & 3.50248 & 50 & 7.84705 \\
    \texttt{Stan} & 100 & 0.7561 & 100 & 0.7261 & 100 & 0.9824 & 20 & 3.5025 & 50 & 7.8471 \\
    %Turing & 100 & 0.33034 & 100 & 0.320111 & 100 & 1.32024 & 20 & 28.2982 & 50 & 11.027 \\
    \texttt{AHMC} & 100 & 0.3303 & 100 & 0.3201 & 100 & 1.3202 & 20 & 28.2982 & 50 & 11.0270 \\
    %\texttt{Stan} & 1000 & 0.761393 & 1000 & 0.708934 & 1000 & 2.26 & 50 & 5.89541 & 200 & 31.3762 \\
    \texttt{Stan} & 1000 & 0.7614 & 1000 & 0.7089 & 1000 & 2.2600 & 50 & 5.8954 & 200 & 31.3762 \\
    %Turing & 1000 & 0.508123 & 1000 & 0.317854 & 1000 & 3.83261 & 50 & 40.0322 & 200 & 33.6125 \\
    \texttt{AHMC} & 1000 & 0.5081 & 1000 & 0.3179 & 1000 & 3.8326 & 50 & 40.0322 & 200 & 33.6125 \\
    \bottomrule
\end{tabular}
}
\end{table}

\section{Discussions}
\subsection{Easy Integration of Other Julia Packages}
\texttt{Bijectors.jl} is used inside \texttt{Turing.jl} to do automatic transformations of constrained variables to run HMC.
E.g. a random variable from $\mathcal{T}runcated(\mathcal{C}auchy(0, 5), 0, \infty)$ is constrained to be positive and 
will be transformed to the real space by the $\log$ function automatically. 

\texttt{CuArrays.jl} could be used with \texttt{AdvancedHMC.jl} to run NUTS on GPUs.
In order to run NUTS using CUDA, 
one only needs to change Line 3 of the demo code 
from \texttt{q0 = randn(D)} to \texttt{q0 = CuArray(randn(D))}, 
assuming \texttt{logdensity\_f} and \texttt{grad\_f} in Line 6 are GPU friendly;
if it is written in pure Julia, it probably supports GPUs acceleration automatically.
\textit{How does it work?}
All arrays in \texttt{AdvancedHMC.jl} are abstractly typed, meaning that the concrete type is deduced at compile time from \texttt{q0}. That is to say, if \texttt{q0} is on the GPU i.e. is a CuArray, all the internal arrays in the NUTS will be too.

\texttt{SoSS.jl} is another PPL in Julia that
uses \texttt{AdvancedHMC.jl} as its backend.
It is easy for PPLs in Julia with different domain specific languages (DSLs)
to use the HMC implementation in \texttt{AdvancedHMC.jl}.

\texttt{DifferentialEquations.jl} is the state-of-the-art numerical differential equations solver package, implemented in pure Julia. As such, its solvers can be employed in Turing models, thus enabling \texttt{AHMC} to perform Bayesian inference in the parameters of differential equation models.

\texttt{Flux.jl} is a deep learning packages in Julia.
Neural models defined by \texttt{Flux.jl} can be directly used in Turing models.
E.g. one can implement a Bayesian neural network in Turing by 
defining priors on the weights of a Flux-based neural network, and 
NUTS can be used to draw samples of the weights.

%%%

\acks{We would like to thank the developers of \texttt{MCMCBenchmarks.jl}, Rob Goedman and Christopher Fisher.
    With this package we ran all benchmarks and generated all plots presented in this poster.}


\bibliography{jmlr-sample}

\newpage

\appendix

\end{document}
